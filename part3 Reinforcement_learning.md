{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a075d08",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Q-LEARNING EXPERIMENT REPORT\n",
    "Student Name: Hassan Rhioui\n",
    "================================================================================\n",
    "\n",
    "EXPERIMENT 1: DEFAULT CONFIGURATION OBSERVATIONS\n",
    "================================================================================\n",
    "\n",
    "Settings from your screenshot:\n",
    "- Grid Size: 10x10 (with default barriers)\n",
    "- Alpha: 0.10\n",
    "- Gamma: 0.90  \n",
    "- Epsilon: 0.10\n",
    "- Iteration shown: 255-297\n",
    "\n",
    "OBSERVATIONS (based on screenshots):\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "From Screenshot 1 (Iteration 255):\n",
    "- Robot position: from (1,2) to (2,2)\n",
    "- Action taken: 1 (DOWN)\n",
    "- Old Q-value: -0.10\n",
    "- New Q-value: -0.20\n",
    "- Reward: -1.0\n",
    "- Episode not done yet\n",
    "\n",
    "From Screenshot 2 (Iteration 296-297):\n",
    "Iteration 296:\n",
    "- Robot position: from (0,0) to (0,0) (hit wall/stayed put)\n",
    "- Action taken: 0 (UP)\n",
    "- Old Q-value: -0.50\n",
    "- New Q-value: -0.95\n",
    "- Reward: -5.0 (wall penalty)\n",
    "\n",
    "Iteration 297:\n",
    "- Robot position: from (0,0) to (0,0)\n",
    "- Action taken: 2 (LEFT)\n",
    "- Old Q-value: 0.00\n",
    "- New Q-value: -0.50\n",
    "- Reward: -5.0\n",
    "\n",
    "Key Observations:\n",
    "1. The robot starts at (0,0) and is still learning\n",
    "2. It receives -5.0 for hitting walls (learns to avoid them)\n",
    "3. Q-values are negative initially (exploration phase)\n",
    "4. After ~300 iterations, still hasn't reached goal consistently\n",
    "\n",
    "================================================================================\n",
    "EXPERIMENT 2: UNDERSTANDING THE Q-VALUES\n",
    "================================================================================\n",
    "\n",
    "What the numbers mean:\n",
    "--------------------------------------------------------------------------------\n",
    "- Negative Q-values (-0.1 to -0.95): The robot is still learning, hasn't found high rewards yet\n",
    "- Wall penalty (-5.0): Teaches robot to avoid obstacles\n",
    "- Action encoding: \n",
    "   0 = UP\n",
    "   1 = DOWN \n",
    "   2 = LEFT \n",
    "   3 = RIGHT\n",
    "\n",
    "Learning Progress:\n",
    "- Early iterations: Random exploration, many wall hits\n",
    "- Mid iterations (255): Starting to move, still getting small penalties\n",
    "- Current state: Building Q-table gradually\n",
    "\n",
    "================================================================================\n",
    "EXPERIMENT 3: HYPERPARAMETER IMPACT ANALYSIS\n",
    "================================================================================\n",
    "\n",
    "Based on configuration (Alpha=0.10, Gamma=0.90, Epsilon=0.10):\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "ALPHA = 0.10 (Learning Rate)\n",
    "- Moderate learning speed\n",
    "- Each new experience updates Q-values by 10%\n",
    "- Balances old knowledge vs new experiences\n",
    "- From logs: oldQ=-0.50 → newQ=-0.95 shows 90% weight on new info\n",
    "\n",
    "GAMMA = 0.90 (Discount Factor)\n",
    "- Values future rewards highly\n",
    "- Robot considers long-term consequences\n",
    "- Good for maze navigation where future steps matter\n",
    "\n",
    "EPSILON = 0.10 (Exploration Rate)\n",
    "- 10% chance of random action\n",
    "- 90% follows learned policy\n",
    "- From iteration 255: followed policy (DOWN)\n",
    "- From iteration 296: random? (UP into wall)\n",
    "\n",
    "================================================================================\n",
    "EXPERIMENT 4: ENVIRONMENT INTERACTION PATTERNS\n",
    "================================================================================\n",
    "\n",
    "From the logs we have:\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Pattern 1: Exploration at (0,0)\n",
    "- Multiple attempts from start position\n",
    "- Tries different actions (UP, LEFT)\n",
    "- Learns that boundaries give -5.0 penalty\n",
    "\n",
    "Pattern 2: Movement after learning\n",
    "- By iteration 255, robot is moving DOWN from (1,2)\n",
    "- Shows some path learning is happening\n",
    "- Still receiving -1.0 step penalties\n",
    "\n",
    "Pattern 3: Q-value evolution\n",
    "- Starting values: 0.00\n",
    "- After wall hits: -0.50, -0.95\n",
    "- After moves: -0.10 → -0.20 (small updates)\n",
    "\n",
    "================================================================================\n",
    "EXPERIMENT 5: Q-TABLE VISUALIZATION ANALYSIS\n",
    "================================================================================\n",
    "\n",
    "From your second screenshot, the Q-Table Graphical View:\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "What you should observe when you click [View Q-Table]:\n",
    "\n",
    "1. Color patterns:\n",
    "   - Green cells = higher Q-values (good actions)\n",
    "   - Red cells = lower Q-values (bad actions)\n",
    "   - Gray hatched cells = walls/obstacles\n",
    "\n",
    "2. Number patterns in each cell:\n",
    "   - Top number: Q-value for UP action\n",
    "   - Bottom number: Q-value for DOWN action\n",
    "   - Left number: Q-value for LEFT action\n",
    "   - Right number: Q-value for RIGHT action\n",
    "\n",
    "3. Robot marker:\n",
    "   - Yellow circle shows current position\n",
    "   - Colored arrow shows last action direction\n",
    "\n",
    "================================================================================\n",
    "REFLECTION QUESTIONS\n",
    "================================================================================\n",
    "\n",
    "1. How does the agent adapt its behavior to reach the goal?\n",
    "   ------------------------------------------------------------------------------\n",
    "   Based on the logs, the agent:\n",
    "   - Starts with random exploration (hitting walls, getting -5.0 penalties)\n",
    "   - Gradually learns which moves lead to better outcomes\n",
    "   - By iteration 255, shows purposeful movement (DOWN from (1,2))\n",
    "   - Updates Q-values based on rewards received\n",
    "   - Will eventually find path to goal after more iterations\n",
    "\n",
    "2. What impact do hyperparameters have on learning?\n",
    "   ------------------------------------------------------------------------------\n",
    "   Alpha (0.10): \n",
    "   - Provides stable, gradual learning\n",
    "   - Each experience has modest impact (10% update)\n",
    "   - Prevents overreacting to single events\n",
    "\n",
    "   Gamma (0.90):\n",
    "   - Robot cares about future rewards\n",
    "   - Will learn to take longer paths if they lead to goal\n",
    "   - Important for maze navigation\n",
    "\n",
    "   Epsilon (0.10):\n",
    "   - Mostly exploits learned knowledge (90%)\n",
    "   - Still explores occasionally (10%)\n",
    "   - Good balance for continued learning\n",
    "\n",
    "3. How do grid layout and obstacles influence learning?\n",
    "   ------------------------------------------------------------------------------\n",
    "   From the logs:\n",
    "   - Walls at boundaries give -5.0 penalty\n",
    "   - Robot quickly learns to avoid these positions\n",
    "   - Starting at (0,0) is challenging (corner with 2 walls)\n",
    "   - Need to learn sequence of moves to navigate around obstacles\n",
    "\n",
    "4. What real-world challenges does this demonstrate?\n",
    "   ------------------------------------------------------------------------------\n",
    "   - Exploration vs Exploitation trade-off (explore new paths vs use known ones)\n",
    "   - Learning from negative feedback (wall penalties)\n",
    "   - Need for many iterations to learn optimal path\n",
    "   - Importance of reward design (goal: +100, walls: -5, steps: -1)\n",
    "   - Cold start problem (starting with no knowledge)\n",
    "\n",
    "================================================================================\n",
    "ETHICAL CONSIDERATIONS\n",
    "================================================================================\n",
    "\n",
    "Identified Risks:\n",
    "--------------------------------------------------------------------------------\n",
    "1. Reward Hacking Risk:\n",
    "   - Robot might find unintended ways to get rewards\n",
    "   - Example: Going in circles to avoid walls\n",
    "   - Mitigation: Carefully designed reward structure\n",
    "\n",
    "2. Safety During Learning:\n",
    "   - In real world, exploration could be dangerous\n",
    "   - Robot might try unsafe actions\n",
    "   - Mitigation: Train in simulation first\n",
    "\n",
    "3. Bias in Learning:\n",
    "   - Robot might favor certain paths\n",
    "   - Could learn suboptimal behaviors\n",
    "   - Mitigation: Multiple training runs, diverse environments\n",
    "\n",
    "Mitigation Strategies:\n",
    "--------------------------------------------------------------------------------\n",
    "1. Use simulation for initial training\n",
    "2. Implement safety constraints\n",
    "3. Regular validation of learned policies\n",
    "4. Human oversight during deployment\n",
    "5. Diverse training environments\n",
    "\n",
    "================================================================================\n",
    "FUTURE EXPERIMENTS TO TRY\n",
    "================================================================================\n",
    "\n",
    "Based on your current setup, try these next:\n",
    "\n",
    "1. Increase Alpha to 0.5 - Watch for faster but possibly unstable learning\n",
    "2. Decrease Epsilon to 0.01 - See if robot stops exploring too early\n",
    "3. Create simple 5x5 maze without obstacles - Observe faster learning\n",
    "4. Add more obstacles - See how complexity affects learning time\n",
    "\n",
    "================================================================================\n",
    "SCREENSHOTS DOCUMENTATION\n",
    "================================================================================\n",
    "\n",
    "Screenshots is attached to this task.\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
